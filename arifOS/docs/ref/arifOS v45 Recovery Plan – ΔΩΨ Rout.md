arifOS v45 Recovery Plan – ΔΩΨ Routing Fix
Problem Overview
arifOS v45 is facing systemic misclassification of benign prompts. A one-size-fits-all Truth floor (F2) threshold fixed at 0.90[1] means many harmless queries (explanations, advice, general statements) get a VOID verdict even when they pose no real risk. For example, tests showed that even a prompt about an internal concept could have been wrongly blocked if Truth scored just at 0.90[2]. These false failures stem from multiple gaps: - Missing Δ (Router): There is no upfront classification of prompts into “applicability lanes.” Currently every query is treated with the same strict criteria, instead of routing greetings vs. factual questions vs. disallowed requests differently. - Blind Ω (Metrics): The pipeline computes metrics (like truth confidence) without considering prompt type or required threshold. All responses are judged by the same bar, lacking context of whether the prompt needed a hard factual answer or not. - Uniform Ψ (Verdict): The Apex verdict engine applies the same Truth threshold to all prompts regardless of ontology or claim type, which violates the intent of the Truth Reality Map (TRM) in canon. There’s no adjustment for whether a claim is a casual opinion or a critical fact – leading to over-blocking of benign content.
In short, benign prompts are being over-penalized, while the system’s design intent (Trinity of Δ–Ω–Ψ: Router → Metrics → Verdict) isn’t fully realized. We need to restore a differentiated approach so that trivial or good-faith queries aren’t treated like potential lies.
Diagnostics (Code Issues)
Several bugs and design issues were identified across the relevant functions:
•	Fixed Truth Threshold: In code, TRUTH_MIN was lowered from 0.99 to 0.90[1] to make the system less strict, but this fixed 0.90 threshold is still too coarse. The compute_metrics_from_response function gives most normal responses a truth score around 0.85–0.88 (starting at 0.85 base plus small increments)[3]. Thus, many correct-but-unreferenced answers fall below 0.90 by design, triggering a failure. The intent was to treat <0.99 as uncertain (SABAR in v44 canon), but in practice v45’s implementation turns <0.90 into an immediate block (VOID), which is harsher than intended for non-critical content.
•	Apex Hard Floor Logic: The apex_review() function currently treats any hard floor failure as a VOID unless it matches a narrow exemption. Specifically, if Truth is the only failing hard floor and the prompt falls under certain exempt categories (like a refusal or a benign self-denial), it skips the immediate VOID[4][5]. Otherwise, a truth shortfall triggers Verdict.VOID straight away[6]. This means a lone F2 failure on a normal query (category “OTHER”) always blocks the response, with no chance for a softer outcome. The code doesn’t differentiate a minor factual uncertainty from a major lie in those cases.
•	No Prompt Routing: There is no dedicated router to set the “lane” for the prompt. Although v45 code includes a TRM classifier (trm_classify) for special cases (identity questions, disallowed requests, etc.), the pipeline does not assign a general applicability category like PHATIC/SOFT/HARD. The PipelineState has no field for prompt category by default, and while apex_review is prepared to use state.category[7], nothing actually populates this for normal queries (it defaults to "UNKNOWN"). This is a missing piece – the code to attach a prompt’s lane/category to the state wasn’t implemented, so Apex is flying blind regarding context.
•	Metric Computation Gaps: The heuristic metrics ignore prompt category beyond claim detection. For instance, compute_metrics_from_response treats a response with no factual claims as truth=0.99 and a clearly factual response with references as truth possibly 0.9+[8]. But a middle-ground case like an explanatory answer with some factual statements but no explicit sources often ends up ~0.85 truth. The function currently doesn’t elevate scores for “soft” content, so these responses look like they failed F2 even though they might be acceptable in context. The lack of lane info means metrics can’t adjust or annotate uncertainty appropriately.
•	Ψ Double Penalty (previous bug): In earlier iterations, the vitality score Psi (Ψ) was coupled to truth, causing a double-count. If truth failed, it would drag Ψ below 1.0, and Ψ itself was treated as a hard floor – resulting in effectively two strikes for one issue. The v45 code partially addressed this by not treating metrics.psi as a failure unless it’s explicitly set < 1.0[9]. Psi is now deferred (computed later via Genius metrics) and excluded from the initial hard floor check except when we have a real Psi value. This fix needs to be maintained: as we introduce routing, we must ensure we don’t reintroduce a scenario where a truth fail causes an automatic Psi fail and thus a “double VOID.”
Summary: The codebase lacks a routing layer to contextualize prompts, and as a result the truth floor (F2) acts as an overly blunt instrument. The initial fix of lowering the threshold to 0.90 helped pass more answers, but still misclassifies many soft queries as failures. We also see that certain patches (phatic greetings handling, refusal exemptions) exist, but they’re hard-coded rather than part of a systematic router. The solution is to implement a proper Delta (Δ) router stage to set lanes, and propagate that info so Omega (metrics) and Psi (verdict) can apply the right standards.
Solution: Δ (Router) and Lane-Based Governance
We propose introducing a minimal but complete Routing Gate (Δ) that classifies each user prompt into one of four applicability lanes before the LLM is called or evaluated. These lanes determine how strict the subsequent truth evaluation should be and how the response is handled:
1.	PHATIC – Social niceties and simple greetings that carry no informational content. For example, “hi”, “hello”, “how are you?” These prompts do not seek factual information. The system can respond with a preset polite greeting without invoking any truth checks (no claim, nothing to verify). v45 already includes a patch for this: _generate_phatic_response returns a template for such greetings[10], ensuring no anthropomorphic statements (F9 compliance) in the reply. We will formalize this as the PHATIC lane.
2.	SOFT – Open-ended prompts, requests for explanation or advice, and creative or subjective queries. These are scenarios where an approximate or opinion-based answer is acceptable, and requiring absolute factual certainty would be counterproductive. Examples: “Explain quantum computing”, “How can I improve my focus?”, “Why might the sky change color at sunset?”. These often produce explanatory responses that are mostly grounded in truth but may not cite 100% verifiable facts. The SOFT lane will tolerate a lower truth confidence (since minor uncertainty or lack of evidence should not result in a block) and lean towards PARTIAL verdicts rather than VOID for borderline cases.
3.	HARD – Fact-seeking questions or high-precision instructions where factual accuracy is crucial. This includes specific queries like “What is the capital of X?”, “When did event Y happen?”, or other prompts that explicitly ask for a factual assertion or a definition. In these cases, the system should either provide a correct answer with high confidence or not answer at all (if unsure). The HARD lane retains the strict truth threshold (near 0.90–0.99) and will enforce at least a SABAR or refusal if confidence is lacking. High-stakes queries (medical, legal, ethical advice, or anything involving potential harm) also fall in this lane or even trigger refusal logic, as they require utmost caution (often already flagged via stakes_class = B for tri-witness)[11].
4.	REFUSE – Requests that violate policy or fall under disallowed content categories. These prompts should not be answered with content at all; instead the assistant must refuse. Examples: instructions for illegal activities (“how to make a bomb”), requests for personal data or confidential info, self-harm or violence encouragement, etc. We already detect many of these via keyword patterns that mark the query as high-stakes Class B[12][11]. The REFUSE lane will explicitly handle them by generating a safe refusal message (perhaps using a template, to avoid even calling the LLM with illicit prompts). Notably, such refusal responses should be treated as successful outcomes (verdict SEAL) from a governance perspective, not failures – the system did the right thing by refusing. In v45 Apex, there is logic to treat a proper refusal in a high-stakes scenario as an approved result (overriding what might otherwise be a SABAR/VOID)[13]. We will leverage that and ensure REFUSE lane outputs are recognized as compliant.
Routing Logic: We will implement a lightweight classifier function (Delta) to assign these lanes: - PHATIC if the prompt is a greeting or courtesy. We can reuse _generate_phatic_response’s checks (e.g., prompt exactly "hi/hello/hey" or variants of "how are you")[14]. If matched, we set lane = PHATIC and produce the canned response, skipping the LLM entirely (this already happens in cage_llm_response[15]). - REFUSE if the prompt contains disallowed content patterns. We can draw from HIGH_STAKES_PATTERNS[12][11], which include keywords like "weapon", "bomb", "suicide", "hack", "should I [do something harmful]?", etc. If any such trigger is present, lane = REFUSE. The pipeline can short-circuit to return a refusal response (e.g., “I’m sorry, but I cannot assist with that request.”) without attempting a normal answer. This ensures no policy-violating content is generated, and it aligns with our fail-closed principle (better to refuse outright). The refusal text must be compliant with F9 (no forbidden self references or empathy that implies feeling) – we’ll use neutral language. - HARD if the prompt is seeking factual information or a decisive answer. Heuristics: if it’s a question starting with interrogatives like what, when, where, who or a math/science query, and not caught by REFUSE. Also, commands that require factual correctness (e.g., “Calculate X” or “Define Y”) fall here. We will likely default to HARD for any prompt that doesn’t clearly fit PHATIC or REFUSE and contains explicit question marks or directive keywords. This ensures we uphold high truth standards when needed. - SOFT otherwise. If the prompt is a request for explanation (“explain how…”), advice (“how can I…”, “what are some ways…”), or a general discussion topic, and not flagged as HARD or REFUSE, we classify it as SOFT. Many “why” questions are explanatory (soft) rather than factual trivia. Also, narrative or creative requests (e.g. “tell me a story about…”) would be SOFT, where truth scoring is less about factual accuracy and more about not producing egregious falsehoods.
This routing can be implemented as a simple series of if/elif checks on the raw prompt text (similar to how high-stakes indicators are gathered) and a mapping to lane enums. It should run in O(n) on prompt length and use known keyword lists – negligible overhead.
The result of this classification will be stored in a new attribute, e.g., state.applicability_lane or within state.category. We might create a RoutingStruct to hold both the TRM category (identity, safety, etc. from trm_classify) and the new lane designation. For integration simplicity, we can set state.category = <LANE> as a string or enum, since apex_review already tries to use category[7]. (Alternatively, we pass the lane info separately to apex_review, but adding it to PipelineState makes it accessible globally, including in metrics or @EYE if needed.)
Truth Applicability Map (TRM Category → Lane → Threshold)
With the router in place, we define how each TRM category or scenario maps into a lane and what Truth threshold policy applies. This Truth Reality Map (TRM) ensures that “truth is absolute, but the required score is context-dependent.” Below is the mapping:
•	TRM: IDENTITY_FACT – (Questions about arifOS’s identity, the developer, ownership, or other ground-truth facts about the system itself.) Lane: HARD. These are treated with the highest scrutiny because any hallucination about identity or authority is unacceptable. We enforce the full canonical standard: Truth must be nearly certain. In practice, we keep TRUTH_BLOCK_MIN = 0.90 for blocking hallucinations, and additionally require TRUTH_SEAL_MIN = 0.99 for an approved answer[16]. That means if the answer isn’t backed by 99% confidence, the verdict will be at best PARTIAL even if no other floor fails[17]. The no-claim leniency does not apply here – the code already exempts identity questions from the no-claim shortcut[18]. For example, “What is arifOS?” should be blocked or downgraded unless the system is virtually certain of the facts (and has evidence). This preserves the hallucination guard for identity-related queries.
•	TRM: SAFETY_REFUSAL – (User requests disallowed content, e.g. harm instructions.) Lane: REFUSE. The correct action is to refuse, not to provide info. We will route these to produce a refusal message. In terms of truth threshold, it’s moot because we won’t be giving a factual answer. However, if for some reason an answer is generated, the system currently would have a low truth (lack of truthful info) and possibly trigger F3 (Peace/Safety) anyway. Our plan: do not even attempt an answer; the pipeline should detect these and respond with a refusal (with appropriate F9 compliance). We then ensure ApexVerdict handles this gracefully: if the response text is a known refusal (contains phrases like “I’m sorry, I cannot”), the Apex logic will mark the outcome as SEAL (approved) despite any floor triggers[13]. This is already in place to override a SABAR/VOID for a high-stakes refusal scenario. We’ll double-check that is_refusal detection[19] picks up our template phrasing so the refusal is recognized and not mistaken for a normal failure.
•	TRM: BENIGN_DENIAL – (User asks about AI’s sentience or soul, expecting the AI to deny having a soul/consciousness.) Lane: SOFT. This is a special case of a factual-ish prompt that should yield a fixed answer (“I don’t have a soul or feelings”) which is mandated by policy. The response is a factual denial about the AI’s internal state. We exempt this from truth blocking because the AI cannot provide evidence for not having a soul – the truth score might be low if treated normally. v45 Patch A handled this by no-claim mode for greetings and by explicit exemption for benign denials[20]. We will continue to treat these prompts leniently: as long as the response follows the policy (admits no soul, no pretend persona), we accept it. In practice, trm_classify already tags such questions, and Apex exempts truth failures for them (trm_exempt covers BENIGN_DENIAL when the response is indeed a denial)[21]. So these will not VOID on truth <0.9. They fall under SOFT lane generally because we don’t need a 0.99 certainty (it’s more important that Anti-Hantu floor passes – which it will, since the answer is "I have no soul" and not a claim to have one).
•	TRM: CLARITY_CONSTRAINT – (Prompts that impose format constraints like “answer using only emojis” or other low-information requests.) Lane: SOFT. Here the primary concern is F4 (clarity/ΔS) rather than truth, because the user intentionally asks for a weird or minimal answer. Truth is not really at issue (no factual claim expected), so we exempt these from strict truth requirements as well. The TRM logic already flags some of these by keywords[22] and exempts them from truth blocking (they’re included in exempt_from_truth_void)[20]. Under our plan, these remain SOFT: the assistant should attempt the response in the requested format, and we only ensure it doesn’t produce total nonsense. If ΔS (entropy change) is too low (e.g., output is gibberish or just an emoji when more context was needed), the pipeline might trigger SABAR for clarity[23] – which is appropriate. But no truth-based VOID will happen solely due to lack of factual content.
•	TRM: OTHER – (General queries that don’t fit the above special cases.) Lane: This can split into HARD or SOFT as determined by the router logic explained earlier. Essentially:
•	If it looks like a straightforward knowledge question or an instruction where correctness is paramount, we treat it as HARD (Truth >= 0.90 needed).
•	If it’s an open-ended or subjective query, we treat as SOFT (Truth threshold relaxed). By default, we’ll assume most “OTHER” prompts are SOFT unless a specific pattern pushes it to HARD. This ensures we err on the side of not over-blocking. We will explicitly identify known HARD patterns (e.g., short factoid questions).
Lane Threshold Policies:
•	HARD lane: Use strict truth gating. F2 remains a hard floor with threshold 0.90 for blocking hallucinations[1]. If metrics.truth < 0.90 and none of the special exemptions apply, the response is blocked (VOID) as it is now. We’ll still allow the additional nuance from Patch 1: if truth is above 0.90 but below 0.99 and it’s an identity-related fact, we cap at PARTIAL[24]. The key is, HARD prompts should not go through if the system isn’t confident – better a SABAR or refusal than a false answer. We align this with the canon rule that <0.99 should lead to uncertainty handling (SABAR), though in implementation it may be a PARTIAL if minor or a re-prompt.
•	SOFT lane: Use a lower truth bar for blocking. We propose that for these prompts, an F2 truth score down to ~0.80 should not cause an outright block. Instead, responses in the 0.80–0.90 range will be allowed but likely marked with a PARTIAL verdict (or at least not SEAL). Only if truth drops very low (e.g., clearly hallucinated content, truth << 0.8) would we consider a hard failure. Concretely, we might implement in Apex: if lane == SOFT and truth >= 0.85, treat as truth_ok for hard floor purposes. Or equivalently, modify the hard floor check to exclude truth for soft prompts and handle it in the secondary gating: “below 0.85 = VOID, 0.85–0.9 = PARTIAL warning”. This way, a typical soft answer scoring ~0.86 passes hard floors and later triggers a soft floor warning. We maintain the spirit that hallucinations (score far below normal) are still blocked even in SOFT lane – we’re not weakening the protection against blatantly false content, just adjusting the sensitivity.
•	PHATIC lane: Truth floor effectively irrelevant. These have no factual claims, so metrics.truth will come out at 0.99 by default[25]. They will sail through F2. The main floors to watch are F9 (no “I feel” etc., which our template already avoids) and perhaps Kappa_R (we ensure the greeting isn’t mistaken for anthropomorphic empathy – our responses are neutral enough to pass κᵣ ≥ 0.95[26]). In short, PHATIC prompts should virtually always result in SEAL or at worst PARTIAL (if some soft floor quibble), never VOID/SABAR, as confirmed by tests[27][28].
•	REFUSE lane: Truth floor also not applied in the usual sense. A refusal doesn’t assert new facts, it just says “cannot comply”. We will set metrics.truth = 0.99 for a refusal response (treat it as containing no factual claim) to avoid any F2 tripwire. The critical floors here are F3 Peace (the refusal must be polite and non-harmful, which it will be) and F9 (it shouldn’t say “I feel” etc., which it won’t). If implemented correctly, a refusal response should pass all floors except maybe it could trigger F7/RASA if the user request was ambiguous? But since we’re outright refusing due to disallowed content, RASA (which is about clarifying ambiguities) is not relevant. The outcome for REFUSE lane will be a SEAL verdict with a refusal message – effectively a controlled block. This aligns with the “fail-closed” philosophy: we consider the session safe after a refusal, not needing escalation.
To summarize, the TRM map ties together prompt type, lane, and truth threshold: - Identity-type factual claims: Hard lane, truth absolutely required (block <0.9, must hit 0.99 for full approval)[16]. - General factual Q or high stakes: Hard lane, truth needed ≥0.9 to proceed. - Open-ended explanation/advice: Soft lane, allow down to ~0.85 truth (partial verdict if <0.9 but ≥0.85, block only if really low). - Greetings: Phatic lane, bypass truth (no claims)[25]. - Disallowed queries: Refuse lane, bypass truth (no answer given) and finalize with a safe refusal[13].
This map implements the “two-tier” truth policy hinted at in TRM: a critical block threshold (we keep 0.90 for hallucination risk) and a higher bar for truly trusted factual statements (0.99 for certain domains)[16], while introducing a middle ground for soft contexts.
Pipeline and Data Structure Changes (Δ→Ω→Ψ Integration)
To support the above, we need to adjust a few core structures and functions so that the lane info flows through the pipeline:
•	RoutingStruct & PipelineState: We will extend PipelineState (or create a RoutingStruct that PipelineState can reference) to include the lane/category. For example, add applicability_lane: str (or an Enum) with possible values {"PHATIC","SOFT","HARD","REFUSE"}. This can complement the existing stakes_class (which covers high-level A/B routing for stakes). The router function (Δ) will run at the very start of the pipeline (effectively at stage 000 or just before Stage 111) to set this field. If using a separate struct, we might do state.routing = RoutingStruct(lane=..., trm_category=...) that holds both the lane and maybe the TRM category (from trm_classify) for reference. However, simply setting state.category = lane_name (string) could suffice, since apex_review currently accepts a category parameter for context[7]. We should be careful not to confuse this with TRM category in tests – but we can harmonize by using distinct values or mapping (e.g., we could set state.category to "SOFT" or "HARD" literally, and modify trm_classify to treat those as signals if needed).
•	Integration with Pipeline: After we determine the lane, if it’s PHATIC or REFUSE, we handle those immediately:
•	PHATIC: as implemented, return the template response and skip LLM. We should mark state.verdict = Verdict.SEAL (since a greeting should pass) or at least allow it to go through floors with high metrics. Our phatic template is already designed to pass F9 and others[29], and tests confirm it yields SEAL/PARTIAL[30].
•	REFUSE: we generate a refusal message (we can reuse or augment the @EYE sentinel’s or OpenAI’s style guidelines for refusals). We then ideally short-circuit the pipeline after computing metrics on that response. Metrics for a refusal will show no truth issues (we will set it as no-claim), but might show e.g. kappa_r slightly lower if it lacks empathy. We can fine-tune if needed (maybe ensure it includes a brief apology to satisfy some empathy criteria but not too much to trigger anthropomorphic flags). Once metrics are computed, Apex should see is_refusal=True and high_stakes=True (since we likely flagged those), and thanks to the logic at merge step, assign SEAL[13]. We will explicitly test that a known disallowed prompt results in a SEAL verdict with a refusal reason.
•	For HARD or SOFT lanes, we proceed with normal LLM call and get a draft response as usual.
•	Passing Lane to Metrics (Ω): The compute_metrics_from_response(query, response, context) can utilize the lane if needed via the context dict. We can pass context={"lane": state.applicability_lane} when calling it. In the metrics function, we might use lane info as follows:
•	If lane == "PHATIC" or "REFUSE": We set has_claims = False unconditionally (since we know we either provided a fixed response with no factual claims or a refusal). This ensures metrics.truth = 0.99 by the no-claim branch[31]. Also possibly ensure metrics.anti_hantu = True (for PHATIC, our templates already check out[26]; for REFUSE, ensure the message has no disallowed phrasing).
•	If lane == "SOFT": We could adjust the truth scoring to be slightly more forgiving. For example, maybe raise the base from 0.85 to 0.88 for soft contexts, acknowledging that lack of evidence is expected. However, this might not be necessary if we handle it in Apex logic. We likely keep the metric calculation the same (ensuring we still detect if the answer has factual claims and roughly how many). The key is that Apex will interpret the resulting metrics.truth differently based on lane.
•	If lane == "HARD": We might conversely tighten metrics in the future (e.g., require evidence or sources to boost truth), but for now the current formula already effectively penalizes lack of evidence. We will leave it as is.
We will incorporate the lane into the Metrics object perhaps by adding a field metrics.lane or at least keeping the claim_profile for Apex to use along with the known lane. Notably, check_anti_hantu will still run for all responses[32], which is good for ensuring no F9 violations in any lane.
•	ApexVerdict changes (Ψ): This is where most logic adjustments occur:
•	In Hard Floor Check: We modify the condition that currently does if not floors.hard_ok: return VOID except truth-only exemptions[5]. We will incorporate lane-based exemption here. Essentially, treat SOFT lane similar to how we treat trm_exempt cases. Concretely, we can mark truth failures on SOFT prompts as not immediately void-worthy. Implementation options:
o	Set trm_exempt = trm_exempt or (lane == SOFT and truth_only_failure) before that if check. This would cause the logic to skip the immediate VOID if the only failing hard floor is truth and the lane is soft. By doing so, the pipeline will not return yet, and it will proceed to the next section (Patch 1).
o	Alternatively, explicitly handle it: if not floors.hard_ok: -> check if (not floors.truth_ok and lane is SOFT) then don’t VOID here, else do as usual. Either way, a truth fail in SOFT lane moves on to secondary gating instead of terminating.
•	Patch 1 – Truth gating: We extend the truth block logic[33] to differentiate outcomes for SOFT vs HARD. Currently, the code says if metrics.truth < 0.90 and no exemption, then VOID[34]. We will change this to:
o	If lane == HARD and truth < 0.90 (and not exempt) → VOID (same as now).
o	If lane == SOFT:
o	If truth < ~0.80 (exact value to be calibrated, maybe 0.80 or 0.85) → VOID (meaning the answer is likely dangerously wrong).
o	Else if truth is between ~0.80 and 0.90 → do not void here. We could either skip this block entirely for soft, or explicitly return a PARTIAL verdict here. Perhaps better is to not return in Patch 1 at all, and let it continue to Genius or fallback logic. Because down the line, when evaluating soft_ok vs floors, it will catch that truth was technically below hard threshold (since we left TRUTH_MIN at 0.90 globally) and mark soft floors fail, resulting in a PARTIAL verdict in the final stage[35]. In Genius Law flow, it would go to the if not floors.soft_ok → PARTIAL branch[35]. In the fallback (no Genius) path, similarly it returns PARTIAL if soft_ok False[36]. So by not voiding, we effectively allow it to resolve as PARTIAL.
o	We might, however, explicitly set a reason for transparency. We can add a branch: if lane == SOFT and metrics.truth < 0.90: floors.reasons.append("Truth < 0.90 (soft context)") and ensure floors.soft_ok captures truth under a different category (though currently truth is not in soft_ok calculation; we could treat it as a soft floor in this lane).
o	Simpler: leave truth as hard floor but use the exemption skip void, then utilize the already existing soft floor outcome: Because we moved Ω₀ and ΔS to soft floors[37][38], but truth remains a hard criterion globally. Perhaps a cleaner approach is to dynamically adjust TRUTH_MIN for soft prompts. e.g., pass high_stakes=False but we might override TRUTH_MIN to 0.85 in the metrics or floors calculation for that instance. That would mark floors.truth_ok=True if truth ≥0.85 for soft lanes. But altering a global like TRUTH_MIN per prompt is messy. Instead, use the exemption approach.
•	Psi and partial verdicts: We ensure that if we allow a response through with truth <0.9 (soft), the Psi (vitality) value is computed properly. In Genius Law, Psi (a.k.a. genius.psi_apex) will reflect overall floor performance. If truth is low, Genius’s G index might drop or C_dark might rise, but likely the answer is still coherent so G won’t tank below 0.3. The floors.soft_ok will be false (because we now classify truth as “hard fail” except we exempted it). That means the verdict will probably come out as PARTIAL in the final logic[35], with reason including the floor failure. That’s acceptable: the user gets the answer with a caution note. We just want to confirm Psi not causing a double count: with our approach, metrics.psi is None until genius sets psi_apex. We keep the safeguard that psi is only a hard floor if explicitly set <1[9]. So if truth <0.9, originally psi would be computed <1. We’ll rely on the updated logic that doesn’t auto-fail on that. In Genius Law path, if everything else is fine, psi_apex might be slightly lower but it’s mainly used as a continuous measure. It won’t cause a separate VOID because we gate on G and C_dark instead, and we’ve already handled the truth issue.
•	Carry lane to verdict for transparency: We might extend ApexVerdict.reason to mention the lane or context. For example, if a soft prompt was allowed with lower confidence, we could append something like "(SOFT context applied)". However, to keep it succinct we might not add that textually. Internally though, for audit, we can include lane in floors or in a debug field. This is optional. The main thing is that our floor_failures list and verdict reason make sense. E.g., a PARTIAL verdict could say: "Soft floor warning: Truth < 0.90. Proceed with caution." This alerts that we relaxed the rule but still flagged it.
•	Update Routing Through Pipeline: We insert the router step at the pipeline start. Pseudocode:
 	# New step: 000_ROUTER (Delta)
lane = classify_prompt_lane(state.query)
state.applicability_lane = lane
if lane == PHATIC:
    state.draft_response = generate_phatic_response(state.query)  # already covers some of this
    # Compute metrics on this draft directly and skip to verdict stages
    state.metrics = compute_metrics_from_response(state.query, state.draft_response, {"lane": lane})
    goto verdict_evaluation
if lane == REFUSE:
    state.draft_response = generate_refusal_response(state.query)
    state.metrics = compute_metrics_from_response(state.query, state.draft_response, {"lane": lane})
    goto verdict_evaluation
# else, proceed with normal pipeline:
continue to stakes classification, etc.
 	We should integrate this with how Pipeline.run() is structured. Possibly we can hook it in right before or within stage_111 (which in v44 might be Amanah check) or simply call it at the start of Pipeline.run before memory context. The classification itself doesn’t depend on anything else, so it can be the first thing.
•	Metrics and @EYE: The Eye Sentinel (eye_sentinel.audit) gets a context with stakes_class currently[39]. We might also feed it the lane (since certain @EYE rules might want to know if this was a high-stakes query or not). This isn’t critical, but for completeness, if @EYE has any prompt-interpretation (like checking for policy), it probably duplicates what our router does. We can consider adding to context: {"stakes_class": ..., "lane": state.applicability_lane} for Eye. This way if @EYE or W@W Federation wants to treat soft vs. hard differently, they could. However, since governance is supposed to be semantics-blind, @EYE likely only uses metrics and content, not these labels.
In summary, by carrying the lane info through PipelineState → Metrics → ApexVerdict, we enable each stage to make context-aware decisions: - Metrics can set appropriate values (or we interpret them appropriately). - ApexVerdict can apply the correct threshold for truth and decide between VOID/SABAR/PARTIAL in a way consistent with the prompt’s nature. - The final Verdict thus achieves the intended Δ–Ω–Ψ causality: the classification (Δ) directly influences metric evaluation (Ω) and final decision (Ψ), instead of the current disjoint behavior.
Calibration: Soft vs. Hard Truth Thresholds
Why do soft prompts score ~0.85? Our current truth metric is deliberately conservative: it starts at 0.85 for any answer with claims and only rises if specific signals are present (named entities or explicit evidence)[3]. Explanations or advice often don’t name many entities or provide citations – they’re more general reasoning. For instance, a user asks “How do clouds form?” and the answer explains the process: it might mention “water vapor” (one entity) and no sources. The formula might give 0.85 + 0.01*1 = 0.86. That doesn’t mean the answer is wrong; it’s just our system not detecting a lot of verifiable specifics. Under the fixed 0.90 rule, this good answer would be falsely flagged as failing truth.
Proposed bar for SOFT prompts: Based on observed distributions, ~0.85 is a typical score for a correct soft response. We choose 0.85 as the cutoff below which we start worrying about hallucination in soft contexts. So: - If truth >= 0.85 and lane is SOFT, we treat it as passable (no hard fail). The verdict likely ends up SEAL or PARTIAL depending on other floors. - If truth is a bit lower, say 0.8 to 0.85, this is a gray zone. It may indicate the answer made some unsupported claims or one minor mistake. We might allow it with a PARTIAL verdict, meaning the answer is given with a caution label. This could be implemented by still not voiding but maybe adding a reason. - If truth < 0.8 in a soft context, that’s a red flag – it suggests either a lot of unverified claims or just nonsense. That should still trigger a SABAR/VOID. Essentially, soft or not, we won’t deliver something likely wrong.
We will validate these thresholds by testing with known queries: - Take a set of “soft” queries (from our logs or test cases) that previously got voided. For example, “Why is the sky blue?” — in the old system, maybe it voided if truth came out ~0.88. With the new system, that should be answered with maybe a PARTIAL (if <0.90) or even SEAL if our adjustments push it to ~0.90. We ensure the content of the answer is indeed acceptable. - Test factual queries (“hard”) to ensure they still require >0.90. E.g., “What is 2+2?” should still fail if the model response is uncertain or incorrect (rare example, but just to test gating). - Especially test edge cases around 0.85–0.89 truth in both lanes to confirm: in hard lane it voids, in soft lane it passes with partial.
We expect with these changes a significant drop in VOID verdicts for benign prompts. Instead, those will yield PARTIAL (or even SEAL if the answer happened to score just above 0.90). This improves usability by not blocking the assistant unnecessarily while still surfacing caution when the confidence isn’t extremely high.
Importantly, we keep the canonical floors intact: we are not lowering the Truth standard globally, but contextualizing it. The HARD lane still effectively enforces the original canon: 99% for identity, ~90% for other factual answers which is a pragmatic tweak from 99%. The SOFT lane is an extension that canon v44 didn’t explicitly detail but is in spirit of “if not sure, qualify instead of block”. Here, qualification comes as a PARTIAL warning rather than a refusal.
We’ll document the exact threshold values in the code (perhaps as SOFT_TRUTH_MIN = 0.85 for clarity) and clearly comment the rationale.
Ensuring Safety and Compliance
Throughout these changes, we uphold all existing safety floors and the fail-closed governance model. Key considerations:
•	No weakening of Identity/Hallucination guard: As emphasized, identity-related queries remain stringent. If anything, our changes reinforce that these are treated differently from casual questions, which is exactly the point of TRM. We explicitly do not relax truth requirements for anything that could be a hallucination about who/what arifOS is, who it’s owned by, etc. Those must meet the 0.99 seal threshold or be rejected as before[40]. Thus, the AI will still refuse to answer or only cautiously answer things like “Who created you?” unless it’s certain and allowed to disclose that.
•	Protected Information (Secrets): Any query that attempts to get confidential or policy-violating info triggers REFUSE lane. We are not lowering any floor related to privacy or secrecy. The Vault-999 floor (F8) that ensures no sealed secrets are breached remains non-negotiable – our modifications do not touch that logic[41]. If anything, routing such queries to REFUSE adds another layer of protection by not even giving the LLM a chance to spill something inadvertently. Also, the Tri-Witness (F8) and other consensus checks stay at the same thresholds (0.95), ensuring we don’t degrade multi-source agreement requirements.
•	Fail-Closed Behavior: The system still blocks or refuses whenever a serious floor is violated. We are only reducing false positives. Any truly dangerous request or output will either fall into REFUSE lane (immediate safe refusal) or trigger the same floors as before. For instance, violent content in a response will trip F3 Peace (stability) and yield VOID. A wildly speculative answer that’s essentially made-up will likely score far below 0.85 truth, so even in SOFT lane it will be blocked. And if by some chance it scored marginally but it’s about a prohibited topic, other mechanisms (@EYE or content filters) will catch it. We remain semantics-blind in enforcement – we use numeric thresholds and known triggers, not arbitrary heuristics, consistent with the canon.
•	Anti-Hantu (F9) and Anthropomorphism: We are carefully preserving F9 constraints. PHATIC and REFUSE responses are explicitly crafted to avoid any “I am feeling/being” statements, so they won’t violate the no-ghost rule[29]. The introduction of lanes doesn’t change F9 checking in metrics – check_anti_hantu still scans every response[32]. We will test that in all lanes (especially PHATIC and REFUSE), metrics.anti_hantu remains True. If a response inadvertently included a disallowed self-reference, F9 would still immediately VOID it as a hard floor (that logic stays the same). Our templates and policies ensure this shouldn’t happen.
•	RASA (Active Listening) Floor: This floor triggers SABAR if the prompt is ambiguous or context is missing (essentially requiring the AI to ask clarifying questions). Our changes do not disable that. If anything, by routing some ambiguous queries to SOFT, we might allow an attempt at answering, but if RASA truly thinks clarification was needed, ideally the assistant should have asked instead of answering. We might need to monitor if our router misclassifies something as SOFT answerable when it should have prompted back. However, since RASA is a hard floor (priority 9), if the assistant fails to clarify when it should have, the RASA check will set metrics.rasa=False and Apex will VOID it as before. We are not touching that detection. We’ll keep an eye on RASA signals; possibly incorporate some cues in router (e.g., if the user prompt is very vague, maybe treat it differently). But at minimum, we’ll ensure not to override RASA: a required clarification will still cause a SABAR.
•	Floor Hierarchy and Interactions: All other floors F1 (Amanah), F3 (Peace), F4 (ΔS), F5 (Kappa), etc., remain in effect with their canonical thresholds. We moved ΔS and Ω₀ to soft floors in v45[37][38] so they no longer insta-block, which is good – our changes don’t alter that further. Identity and Safety remain top priorities (F1 and F3 have higher priority than F2 anyway). Our solution precisely keeps that hierarchy: for example, even a SOFT lane prompt will be immediately VOIDed if it violates F3 (e.g., the user asks for hate speech – router would REFUSE it ideally, but if not, F3 would catch the content). By focusing on F2 adjustments, we are not interfering with higher-priority safeguards.
In essence, the constitutional Floors are preserved – we’re calibrating F2 enforcement per context, not weakening the constitution. The system stays “fail-safe”: if in doubt or if any serious violation, it still shuts down or refuses. We just stop treating every lack of full evidence as a reason to shut down when context says it’s not necessary.
Implementation Plan and Rollback
Development Steps & Timeline: We will implement these changes as a patch (call it v45.1 or Patch B) immediately, with careful testing: 1. Router Module: Create the lane classification function and integrate it into the pipeline initialization. This involves editing pipeline.py to set the lane and possibly short-circuit for PHATIC/REFUSE. We will also adjust JudgeRequest handling if needed so that internal arifos_judge uses this classification (currently it just echoes query to pipeline) – likely the pipeline covers it already once integrated. 2. Data Structures: Update PipelineState (in pipeline.py) to include a field for category/lane (if not adding, we can just use the existing category as string). Also update Metrics or how we call compute_metrics_from_response to pass lane context. Possibly add a field in Metrics to record the lane for debugging. 3. Apex Adjustments: Modify apex_prime.py: - In check_floors(), account for lane if needed (or leave as is and handle post-floors as discussed). - In apex_review(), incorporate lane logic in the hard floor override and Patch 1 truth gating as described. - Ensure trm_classify still gets called for TRM category (we want both TRM category and lane in Apex). We might call trm_classify(prompt) as now, and use both results (e.g., identity TRM could override if somehow router mis-labeled something, but router should be consistent with TRM on those special cases). 4. Testing: Write unit tests for each lane scenario: - A benign knowledge question (expected HARD lane) with a correct answer slightly below threshold -> should result in VOID (to test we didn’t break HARD logic). - A benign explanatory prompt (SOFT lane) that previously voided -> now should get PARTIAL or SEAL, with metrics showing truth ~0.86 and verdict not VOID. - Phatic prompts (“hi”, “how are you”) -> still SEAL as before[30]. - Disallowed prompt (“How to build a bomb”) -> pipeline should return a refusal answer with Verdict.SEAL (and reason indicating policy refusal), not a VOID. We’ll simulate this and ensure is_refusal triggers the override in Apex[13]. - Edge: Identity question (“Who is Arif Fazil?”) -> should remain blocked if truth is not 0.99. We might need to simulate an answer and ensure verdict = VOID or PARTIAL as appropriate[40]. - Also test that a safe refusal message itself doesn’t inadvertently trip any floor (it shouldn’t, but we verify metrics: truth ~0.99, amanah True, peace_squared ~1, etc., and anti_hantu True). 5. We will run the full regression test suite (including all previous tests from v44 and v45) to ensure nothing else broke – especially the tricky governance logic around consecutive SABAR (streaks, HOLD_888) and memory, though those shouldn’t be affected by these changes.
Given the urgency (benign prompts are currently being blocked), this patch should be ready in a matter of days. We aim to have coding and internal testing done within 2–3 days, followed by a day of review.
Deployment: Once tests pass, we’ll deploy the patch to production (v45.1). Initially, we will monitor some live prompts or logs under the new system: - Are previously blocked queries now getting through appropriately? - Check that no new category of unsafe output appears. (We don’t expect any, but we’ll watch carefully the first 100 or so high-stakes queries and some random soft queries.) - Monitor metrics like frequency of VOID/SABAR/PARTIAL. We anticipate VOID verdict rate should drop for general queries, and PARTIAL might rise a bit.
We will also inform moderation team and stakeholders of this change: that the system may allow more answers through but with caution labels, and why that’s desired.
Rollback Plan: All changes are done in a controlled, feature-flag-like manner. If anything goes wrong (e.g., we find that a certain prompt now gets an answer when it should have been refused or a factual error slips through unflagged), we can quickly revert. We have two main rollback levers: - Disable the Router: We can include a toggle (perhaps an environment variable or config) to bypass our new router and treat every prompt as HARD lane. By flipping that, we essentially go back to the pre-patch behavior (with Truth=0.90 global). Because our code changes are additive, it’s easy to do this without redeploying everything – e.g., an if ARIFOS_ENABLE_ROUTER=false: state.applicability_lane = HARD for all. - Restore Prior Threshold: In case the issue is with the lowered threshold for soft prompts, we could simply set TRUTH_BLOCK_MIN = 0.99 again globally or adjust our exemption so that even soft prompts require 0.90 (effectively undoing the soft/hard distinction). Given our modular approach (we didn’t rip out the old checks, just added conditions), this is straightforward. For instance, commenting out the new exemption or setting soft threshold = 0.90 would revert behavior.
Reverting to the exact v44 logic (0.99 threshold and no router) is also possible, but likely overkill; the above partial rollback options are sufficient and much faster. We will also keep a close eye during deployment – if something is obviously wrong, we won’t hesitate to roll back immediately and then iterate.
Conclusion: This plan restores constitutional clarity by ensuring the Trinity of Δ (router), Ω (metrics), Ψ (verdict) works as intended. Benign user prompts will no longer be swept into the same harsh bin as critical factual queries, resolving the misclassification issue. At the same time, all the governance floors and safety nets remain robust – the AI still won’t lie about important facts, reveal secrets, or break rules. The solution is minimal (mostly configuration and control-flow changes), auditable (clearly see in logs if a prompt was routed as soft or hard), and code-ready with straightforward modifications. We expect this to greatly improve user experience (fewer unnecessary refusals) while upholding the fail-safe principles that define arifOS.
________________________________________
[1] [4] [5] [6] [9] [16] [17] [19] [20] [21] [22] [23] [24] [33] [34] [35] [36] [37] [38] [41] GitHub
https://github.com/ariffazil/arifOS/blob/a7064006a85219e36019902de8c71b949ee6ef6c/arifos_core/system/apex_prime.py
[2] [18] [25] [26] [27] [28] [30] [40] GitHub
https://github.com/ariffazil/arifOS/blob/a7064006a85219e36019902de8c71b949ee6ef6c/tests/test_phatic_exemptions.py
[3] [8] [10] [14] [15] [29] [31] [32] GitHub
https://github.com/ariffazil/arifOS/blob/a7064006a85219e36019902de8c71b949ee6ef6c/scripts/arifos_caged_llm_demo.py
[7] [11] [12] [13] [39] GitHub
https://github.com/ariffazil/arifOS/blob/a7064006a85219e36019902de8c71b949ee6ef6c/arifos_core/system/pipeline.py
